{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cce85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from logging import getLogger\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Tuple,\n",
    "    Optional,\n",
    "    Union,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bc3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d1d374",
   "metadata": {},
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84c9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d369079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "Dialog = Sequence[Message]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    special_tokens: Dict[str, int]\n",
    "\n",
    "    num_reserved_special_tokens = 256\n",
    "\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the Tokenizer with a Tiktoken model.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The path to the Tiktoken model file.\n",
    "        \"\"\"\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [\n",
    "            f\"<|reserved_special_token_{i}|>\"\n",
    "            for i in range(5, self.num_reserved_special_tokens - 5)\n",
    "        ]\n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens,\n",
    "        )\n",
    "        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n",
    "\n",
    "        self.n_words: int = self.model.n_vocab\n",
    "        # BOS / EOS token IDs\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encodes a string into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            s (str): The input string to be encoded.\n",
    "            bos (bool): Whether to prepend the beginning-of-sequence token.\n",
    "            eos (bool): Whether to append the end-of-sequence token.\n",
    "            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n",
    "            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n",
    "\n",
    "        Returns:\n",
    "            list[int]: A list of token IDs.\n",
    "\n",
    "        By default, setting disallowed_special=() encodes a string by ignoring\n",
    "        special tokens. Specifically:\n",
    "        - Setting `disallowed_special` to () will cause all text corresponding\n",
    "          to special tokens to be encoded as natural text (insteading of raising\n",
    "          an error).\n",
    "        - Setting `allowed_special` to \"all\" will treat all text corresponding\n",
    "          to special tokens to be encoded as special tokens.\n",
    "        \"\"\"\n",
    "        assert type(s) is str\n",
    "\n",
    "        # The tiktoken tokenizer can handle <=400k chars without\n",
    "        # pyo3_runtime.PanicException.\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "\n",
    "        # https://github.com/openai/tiktoken/issues/195\n",
    "        # Here we iterate over subsequences and split if we exceed the limit\n",
    "        # of max consecutive non-whitespace or whitespace characters.\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "\n",
    "        substrs = (\n",
    "            substr\n",
    "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
    "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
    "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "        )\n",
    "        t: List[int] = []\n",
    "        for substr in substrs:\n",
    "            t.extend(\n",
    "                self.model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs into a string.\n",
    "\n",
    "        Args:\n",
    "            t (List[int]): The list of token IDs to be decoded.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n",
    "        return self.model.decode(cast(List[int], t))\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_whitespaces_or_nonwhitespaces(\n",
    "        s: str, max_consecutive_slice_len: int\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n",
    "        consecutive whitespaces or consecutive non-whitespaces.\n",
    "        \"\"\"\n",
    "        current_slice_len = 0\n",
    "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "        slice_start = 0\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            is_now_space = s[i].isspace()\n",
    "\n",
    "            if current_slice_is_space ^ is_now_space:\n",
    "                current_slice_len = 1\n",
    "                current_slice_is_space = is_now_space\n",
    "            else:\n",
    "                current_slice_len += 1\n",
    "                if current_slice_len > max_consecutive_slice_len:\n",
    "                    yield s[slice_start:i]\n",
    "                    slice_start = i\n",
    "                    current_slice_len = 1\n",
    "        yield s[slice_start:]\n",
    "\n",
    "\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer: Tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message: Message) -> List[int]:\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_message(self, message: Message) -> List[int]:\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def encode_dialog_prompt(self, dialog: Dialog) -> List[int]:\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n",
    "        for message in dialog:\n",
    "            tokens.extend(self.encode_message(message))\n",
    "        # Add the start of an assistant message for the model to complete.\n",
    "        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd23a9",
   "metadata": {},
   "source": [
    "#### Create Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b8e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "    use_scaled_rope: bool = True\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543acce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73aa0ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pprint(\u001b[43mmodel_args\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_args' is not defined"
     ]
    }
   ],
   "source": [
    "pprint(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad59b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # model_parallel_size = fs_init.get_model_parallel_world_size()\n",
    "        model_parallel_size = 1\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).to(device)\n",
    "\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                args.max_batch_size,\n",
    "                args.max_seq_len,\n",
    "                self.n_local_kv_heads,\n",
    "                self.head_dim,\n",
    "            )\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xv)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        keys = repeat_kv(\n",
    "            keys, self.n_rep\n",
    "        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "        values = repeat_kv(\n",
    "            values, self.n_rep\n",
    "        )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        keys = keys.transpose(1, 2)  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        values = values.transpose(\n",
    "            1, 2\n",
    "        )  # (bs, n_local_heads, cache_len + seqlen, head_dim)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "        ffn_dim_multiplier: Optional[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        start_pos: int,\n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = torch.nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            params.dim // params.n_heads,\n",
    "            params.max_seq_len * 2,\n",
    "            params.rope_theta,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "\n",
    "        h = self.tok_embeddings(tokens)\n",
    "\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "            # When performing key-value caching, we compute the attention scores\n",
    "            # only for the new sequence. Thus, the matrix of scores is of size\n",
    "            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
    "            # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
    "            mask = torch.hstack(\n",
    "                [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
    "            ).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h).float()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa12455",
   "metadata": {},
   "source": [
    "#### Create LLama Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "class Llama:\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.formatter = ChatFormat(tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "        seed: int = 1,\n",
    "    ) -> \"Llama\":\n",
    "        assert (\n",
    "            1 <= max_seq_len <= 8192\n",
    "        ), f\"max_seq_len must be between 1 and 8192, got {max_seq_len}.\"\n",
    "        assert os.path.isdir(\n",
    "            ckpt_dir\n",
    "        ), f\"Checkpoint directory '{ckpt_dir}' does not exist.\"\n",
    "        assert os.path.isfile(\n",
    "            tokenizer_path\n",
    "        ), f\"Tokenizer file '{tokenizer_path}' does not exist.\"\n",
    "\n",
    "        # seed must be the same in all processes\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        start_time = time.time()\n",
    "        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "\n",
    "        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n",
    "\n",
    "        ckpt_path = checkpoints[0]\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "\n",
    "        model_args: ModelArgs = ModelArgs(\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "\n",
    "        assert model_args.vocab_size == tokenizer.n_words\n",
    "\n",
    "        # if torch.cuda.is_bf16_supported():\n",
    "        #     torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n",
    "        # else:\n",
    "        #     torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "\n",
    "        model = Transformer(model_args)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ):\n",
    "        params = self.model.params\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= params.max_seq_len\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=device)\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=device)\n",
    "        input_text_mask = tokens != pad_id\n",
    "\n",
    "        if min_prompt_len == total_len:\n",
    "            logits = self.model.forward(tokens, prev_pos)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "\n",
    "        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens)).to(device)\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                torch.isin(next_token, stop_tokens)\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to after eos tok if any\n",
    "            for stop_token in self.tokenizer.stop_tokens:\n",
    "                try:\n",
    "                    eos_idx = toks.index(stop_token)\n",
    "                    toks = toks[:eos_idx]\n",
    "                    probs = probs[:eos_idx] if logprobs else None\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        dialogs: List[Dialog],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "    ):\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = self.model.params.max_seq_len - 1\n",
    "\n",
    "        prompt_tokens = [\n",
    "            self.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n",
    "        ]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": self.tokenizer.decode(t),\n",
    "                    },\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [\n",
    "            {\n",
    "                \"generation\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": self.tokenizer.decode(t),\n",
    "                },\n",
    "            }\n",
    "            for t in generation_tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a407e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/Users/diab/Desktop/Projects/VoT/.llama/checkpoints/Llama3.2-1B\"\n",
    "tokenizer_path = (\n",
    "    \"/Users/diab/Desktop/Projects/VoT/.llama/checkpoints/Llama3.2-1B/tokenizer.model\"\n",
    ")\n",
    "max_batch_size = 32\n",
    "max_seq_len = 1024\n",
    "max_gen_len = 1024\n",
    "\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to leave the world better than we found it. - Mark Twain\n",
      "The best way to find yourself is to lose yourself in the service of others. - Mahatma Gandhi\n",
      "I believe the meaning of life is to leave the world better than we found it. - Mark Twain\n",
      "The best way to find\n",
      "\n",
      "==================================\n",
      "\n",
      "Simply put, the theory of relativity states that \n",
      "> 1) light travels at a constant speed and 2) time and space are relative to the observer.\n",
      "The first law of relativity states that the speed of light is constant in a vacuum and is independent of the motion of the source or observer. The second law of relativity states that time and space are relative to\n",
      "\n",
      "==================================\n",
      "\n",
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "    Hi everyone,\n",
      "\n",
      "    I just \n",
      ">  wanted to give a quick update on the new website and how we are\n",
      "    working on the next phase of development.  The new website is live and\n",
      "    available at: https://www.uxmanagers.org/uxmanagers.org/\n",
      "\n",
      "    The new website is a work in progress.  The next phase\n",
      "\n",
      "==================================\n",
      "\n",
      "Translate English to French:\n",
      "\n",
      "    sea otter => loutre de mer\n",
      "    peppermint => menthe poivrée\n",
      "    plush girafe => girafe peluche\n",
      "    cheese =>\n",
      ">  fromage\n",
      "    kitten => chaton\n",
      "    snail => coquille\n",
      "    ouch => mordant\n",
      "    ouch! => mordant !\n",
      "    mordant => mordant\n",
      "    mordant! => mordant !\n",
      "\n",
      "Translate French to English:\n",
      "\n",
      "    loutre de mer\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompts: List[str] = [\n",
    "#     # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "#     \"I believe the meaning of life is\",\n",
    "#     \"Simply put, the theory of relativity states that \",\n",
    "#     \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "#     Hi everyone,\n",
    "\n",
    "#     I just \"\"\",\n",
    "#     # Few shot prompt (providing a few examples before asking model to complete more);\n",
    "#     \"\"\"Translate English to French:\n",
    "\n",
    "#     sea otter => loutre de mer\n",
    "#     peppermint => menthe poivrée\n",
    "#     plush girafe => girafe peluche\n",
    "#     cheese =>\"\"\",\n",
    "# ]\n",
    "# results = generator.text_completion(prompts, max_gen_len=max_gen_len)\n",
    "# for prompt, result in zip(prompts, results):\n",
    "#     print(prompt)\n",
    "#     print(f\"> {result['generation']}\")\n",
    "#     print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42418b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c1178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3622da9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m      1\u001b[39m dialogs: List[Dialog] = [\n\u001b[32m      2\u001b[39m     [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwhat is the recipe of mayonnaise?\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m      3\u001b[39m     [\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     ],\n\u001b[32m     29\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m results = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dialog, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dialogs, results):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m dialog:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 204\u001b[39m, in \u001b[36mLlama.chat_completion\u001b[39m\u001b[34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[39m\n\u001b[32m    199\u001b[39m     max_gen_len = \u001b[38;5;28mself\u001b[39m.model.params.max_seq_len - \u001b[32m1\u001b[39m\n\u001b[32m    201\u001b[39m prompt_tokens = [\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mself\u001b[39m.formatter.encode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[32m    203\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m generation_tokens, generation_logprobs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    213\u001b[39m         {\n\u001b[32m    214\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[32m    222\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/VoT/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[39m\n\u001b[32m     81\u001b[39m min_prompt_len = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m prompt_tokens)\n\u001b[32m     82\u001b[39m max_prompt_len = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m prompt_tokens)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m max_prompt_len <= params.max_seq_len\n\u001b[32m     84\u001b[39m total_len = \u001b[38;5;28mmin\u001b[39m(params.max_seq_len, max_gen_len + max_prompt_len)\n\u001b[32m     86\u001b[39m pad_id = \u001b[38;5;28mself\u001b[39m.tokenizer.pad_id\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dialogs: List[Dialog] = [\n",
    "    [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\"\\\n",
    "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
    "\n",
    "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
    "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
    "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
    "\n",
    "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
    "        {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Always answer with emojis\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
    "    ],\n",
    "]\n",
    "\n",
    "results = generator.chat_completion(\n",
    "    dialogs,\n",
    "    max_gen_len=max_gen_len,\n",
    ")\n",
    "\n",
    "for dialog, result in zip(dialogs, results):\n",
    "    for msg in dialog:\n",
    "        print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n",
    "    print(\n",
    "        f\"> {result['generation']['role'].capitalize()}: {result['generation']['content']}\"\n",
    "    )\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do:\n",
    "\n",
    "# Create ModelArgs class - Done\n",
    "# Create Transformer class - Done\n",
    "# Create Tokenizer class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vot (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
